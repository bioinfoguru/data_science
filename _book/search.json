[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science",
    "section": "",
    "text": "Python has several libraries for parsing and visualization of data. Here, we’ll learn some of these libraries.\nTo learn more about Bioinformatics learning resources, please visit bioinfo.guru.\nFor any query or feedback related to Python Book, please reach out to the author at manish@bioinfo.guru"
  },
  {
    "objectID": "Covid19.html",
    "href": "Covid19.html",
    "title": "1  Covid19 data analysis",
    "section": "",
    "text": "Data analysis often requiring pre-processing of the data before the actual analysis. One must have a good understanding of not only the contents of the data but also must pay attention to the layout of the data. Many a times it is imperative to change the layout of the data to perform the analysis that we intend to do. We’ll now look at some of these concepts in action using the Covid19 data avaiable from Our World in Data. The csv file is available here.\nThis is a large dataset having thousands of rows and multiple columns. First, lets check what all information we have in this dataset i.e. what all columns are there. This can be achived by columns attribute of the dataframe. Please refer to the Our World in Data website for more information about the contents of this dataset.\nNext, we need to ask question(s) that we would like to be addressed through the analysis of this data. One such question could be – which are the top 10 countries with the highest number of total Covid19 cases? Given our dataset, we need to breakdown this problem into different steps. First get all the rows with the most current date in the date column. For this example we’ll retain only the location and total_cases columns although we can certain retain the entire dataset as well. An important point to note here is that that the location column need not have only the names of countries; there are certain groups name that are present as well, which we need to filter out. To filter these non-country location we need to prepare a list of all such group names (viz. World, Asia, etc). This list would then be passed to the isin function to make the required selection from the original dataframe. Next, using the nlargest function we can get the top n values for the total_cases column. The total_cases are formated to show the values with commas.\nTo get countries with the maximum number of cases per continent, we can use the groupby function to group the dataframe by continent column. Then using the idmax function the indices of the highest value for the total_cases column is retrieved. To get the final list, use these indices to get rows by location i.e. with the loc attribute of the dataframe.\nBar plot of top 10 countries."
  },
  {
    "objectID": "Covid19.html#changing-axis-scale",
    "href": "Covid19.html#changing-axis-scale",
    "title": "1  Covid19 data analysis",
    "section": "1.1 Changing axis scale",
    "text": "1.1 Changing axis scale\nOn a linear scale there is a fixed increment at regular intervals. The positioning of the tick marks on the axis for a graph with linear scale is calculated by addition. Whereas, on a log scale the ticks on the axis are marked using multiplication factor. This makes log scale graphs less intuitive to understand the underlying information and therefore it requires some training to parse graphs with log scale. Let’s plot some data to get a clarity on this distinction between these scales. For this example, we’ll plot exponential curve i.e. the values would be like 1,2,4,8,16,32…etc. The graphs below shows this trend plotted with y-axis having a linear (left) and a log (right) scale. Notice how different the two curves look! The log scale graph is useful in interpreting the pattern of the growth such that an exponential growth on a log scale would result in a straight line.\n\nimport numpy as np\nN=1\nx = range(1,11)\ny = []\nfor a in x:\n    N = N*2\n    y.append(N)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2,figsize=(9,3))\nax1.plot(x,y)\nax2.plot(x,y)\nax2.set_yscale('log')\nax1.set_title(\"Linear Scale\")\nax2.set_title(\"Log Scale\")\nplt.show()\n\n\n\n\nLet’s plot the emergence of new cases on linear and log scales.\n\nfig,ax=plt.subplots(1,2,figsize=(12,3))\n\ndf1[df1[\"location\"]==\"India\"].plot(x=\"date\", y=\"total_cases\",ax=ax[0])\ndf1[df1[\"location\"]==\"United States\"].plot(x=\"date\", y=\"total_cases\",ax=ax[0])\ndf1[df1[\"location\"]==\"France\"].plot(x=\"date\", y=\"total_cases\",ax=ax[0])\nax[0].legend([\"India\",\"United States\", \"France\"])\nax[0].set_ylabel(\"Total Cases\")\nax[0].set_title(\"Linear\")\n\ndf1[df1[\"location\"]==\"India\"].plot(x=\"date\", y=\"total_cases\",ax=ax[1])\ndf1[df1[\"location\"]==\"United States\"].plot(x=\"date\", y=\"total_cases\",ax=ax[1])\ndf1[df1[\"location\"]==\"France\"].plot(x=\"date\", y=\"total_cases\",ax=ax[1])\nplt.legend([\"India\",\"United States\", \"France\"])\nax[1].legend([\"India\",\"United States\", \"France\"])\nax[1].set_ylabel(\"Total Cases\")\nax[1].set_yscale(\"log\")\nax[1].set_title(\"Log\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Covid19.html#function-to-plot-total-cases",
    "href": "Covid19.html#function-to-plot-total-cases",
    "title": "1  Covid19 data analysis",
    "section": "1.2 Function to plot total cases",
    "text": "1.2 Function to plot total cases\nWe can write a function to plot the time series of total cases for any location(s). This function would be define with *args to hold a list of locations that need to be plotted.\n\ndef plot_country(*args):\n    fig,ax=plt.subplots(figsize=(8,3))\n    for c in args:\n        df1[df1[\"location\"]==c].plot(x=\"date\", y=\"total_cases\",ax=ax)\n    plt.legend(args)\n    plt.show()\n\nplot_country(\"India\",\"United States\")\n\n\n\n\nThe visualize emergence of new cases over time, we need to plot the new_cases for a given location. This kind of plot would give us an idea about the “waves” of the pandemic.\n\nfig,ax=plt.subplots(figsize=(8,3))\ndf1[df1[\"location\"]==\"India\"].plot(x=\"date\", y=\"new_cases\",ax=ax)\nplt.ylabel(\"Number of cases\")\nplt.legend([\"India\"])\nplt.show()\n\n\n\n\nThe analyze a particular country in detail, a subset of the dataframe can be created having data for only that country. In the example below we’ll make a dataframe for location “India” and then we’ll find out number of monthly cases in a specific year. For this we’ll first need to change the datatype of the “date” column to pandas datatime datatype. The datatypes for all the columns can be checked by used the dtypes attribute for the dataframe.\n\ndf_India = df1[df1[\"location\"]==\"India\"]\ndf_India.head()\n\n\n\n\n\n  \n    \n      \n      iso_code\n      continent\n      location\n      date\n      total_cases\n      new_cases\n      new_cases_smoothed\n      total_deaths\n      new_deaths\n      new_deaths_smoothed\n      ...\n      female_smokers\n      male_smokers\n      handwashing_facilities\n      hospital_beds_per_thousand\n      life_expectancy\n      human_development_index\n      excess_mortality_cumulative_absolute\n      excess_mortality_cumulative\n      excess_mortality\n      excess_mortality_cumulative_per_million\n    \n  \n  \n    \n      88211\n      IND\n      Asia\n      India\n      2020-01-30\n      1.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      88212\n      IND\n      Asia\n      India\n      2020-01-31\n      1.0\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      88213\n      IND\n      Asia\n      India\n      2020-02-01\n      1.0\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      88214\n      IND\n      Asia\n      India\n      2020-02-02\n      2.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      88215\n      IND\n      Asia\n      India\n      2020-02-03\n      3.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 67 columns\n\n\n\n\nprint(\"The data type for the date column is\", df_India['date'].dtype)\ndf_India['date'] = pd.to_datetime(df_India['date'])\nprint(\"The data type for the date column is\", df_India['date'].dtype)\n\nThe data type for the date column is object\nThe data type for the date column is datetime64[ns]\n\n\n\ndf_India_2022 = df_India[(df_India[\"date\"] >= '2022-01-01')]\ndf_India_2022.dtypes\n\niso_code                                           object\ncontinent                                          object\nlocation                                           object\ndate                                       datetime64[ns]\ntotal_cases                                       float64\n                                                ...      \nhuman_development_index                           float64\nexcess_mortality_cumulative_absolute              float64\nexcess_mortality_cumulative                       float64\nexcess_mortality                                  float64\nexcess_mortality_cumulative_per_million           float64\nLength: 67, dtype: object\n\n\n\ndf_months = df_India_2022.groupby(df_India_2022['date'].dt.strftime('%B'))[\"new_cases\"].sum().to_frame().reset_index()\ndf_months\n\n\n\n\n\n  \n    \n      \n      date\n      new_cases\n    \n  \n  \n    \n      0\n      April\n      53413.0\n    \n    \n      1\n      August\n      400064.0\n    \n    \n      2\n      February\n      1461546.0\n    \n    \n      3\n      January\n      6607920.0\n    \n    \n      4\n      July\n      567041.0\n    \n    \n      5\n      June\n      308402.0\n    \n    \n      6\n      March\n      94730.0\n    \n    \n      7\n      May\n      81644.0\n    \n    \n      8\n      September\n      33322.0\n    \n  \n\n\n\n\nTo sort the dataframe by month, we need to first change the data type of the months column to catagorical since the default data type of this column (object) would lead to sorting by alphabetical order. We would like to sort this column by the order that we see in a calender (which, of course, is not alphabetical).\n\ndf_months.sort_values(by=\"date\",inplace=True, ignore_index=True)\ndf_months_1 = df_months.style.set_caption(\"Date as object data type\")\ndisplay(df_months_1)\nmonths = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\ndf_months['date'] = pd.Categorical(df_months['date'], categories=months, ordered=True)\ndf_months.sort_values(by=\"date\",inplace=True, ignore_index=True)\ndf_months_2 = df_months.style.set_caption(\"Date as catagorical data type\")\ndisplay(df_months_2)\n\n\n\n\n\n\n  Date as object data type\n  \n    \n       \n      date\n      new_cases\n    \n  \n  \n    \n      0\n      April\n      53413.000000\n    \n    \n      1\n      August\n      400064.000000\n    \n    \n      2\n      February\n      1461546.000000\n    \n    \n      3\n      January\n      6607920.000000\n    \n    \n      4\n      July\n      567041.000000\n    \n    \n      5\n      June\n      308402.000000\n    \n    \n      6\n      March\n      94730.000000\n    \n    \n      7\n      May\n      81644.000000\n    \n    \n      8\n      September\n      33322.000000\n    \n  \n\n\n\n\n\n\n  Date as catagorical data type\n  \n    \n       \n      date\n      new_cases\n    \n  \n  \n    \n      0\n      January\n      6607920.000000\n    \n    \n      1\n      February\n      1461546.000000\n    \n    \n      2\n      March\n      94730.000000\n    \n    \n      3\n      April\n      53413.000000\n    \n    \n      4\n      May\n      81644.000000\n    \n    \n      5\n      June\n      308402.000000\n    \n    \n      6\n      July\n      567041.000000\n    \n    \n      7\n      August\n      400064.000000\n    \n    \n      8\n      September\n      33322.000000\n    \n  \n\n\n\n\n\n\nfig,ax=plt.subplots()\ndf_months.plot(kind=\"bar\",xlabel=\"date\", ax=ax)\nax.set_xticklabels([\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \\\n          \"July\", \"August\", \"September\"])\nplt.ylabel(\"Number of cases\")\nplt.show()\n\n\n\n\n\ndf_India_2022[\"date\"] = df_India_2022[\"date\"].dt.strftime('%B')\nmonths = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\"July\", \"August\", \"September\"]\ndf_India_2022['date'] = pd.Categorical(df_India_2022['date'], categories=months, ordered=True)\ndf_India_2022.sort_values(by=\"date\",inplace=True, ignore_index=True)\ndf_India_2022.head()\n\n\n\n\n\n  \n    \n      \n      iso_code\n      continent\n      location\n      date\n      total_cases\n      new_cases\n      new_cases_smoothed\n      total_deaths\n      new_deaths\n      new_deaths_smoothed\n      ...\n      female_smokers\n      male_smokers\n      handwashing_facilities\n      hospital_beds_per_thousand\n      life_expectancy\n      human_development_index\n      excess_mortality_cumulative_absolute\n      excess_mortality_cumulative\n      excess_mortality\n      excess_mortality_cumulative_per_million\n    \n  \n  \n    \n      0\n      IND\n      Asia\n      India\n      January\n      34889132.0\n      27553.0\n      14618.571\n      481770.0\n      284.0\n      298.286\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      IND\n      Asia\n      India\n      January\n      41469499.0\n      167059.0\n      238613.857\n      496242.0\n      1192.0\n      825.714\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      IND\n      Asia\n      India\n      January\n      41302440.0\n      209918.0\n      251301.714\n      495050.0\n      959.0\n      743.143\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      IND\n      Asia\n      India\n      January\n      41092522.0\n      234281.0\n      265036.857\n      494091.0\n      893.0\n      668.857\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      IND\n      Asia\n      India\n      January\n      40858241.0\n      235532.0\n      279215.714\n      493198.0\n      871.0\n      616.286\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 67 columns\n\n\n\n\nax = df_India_2022.boxplot(column=\"new_cases\",by=\"date\")\nplt.title(\"\")\nplt.show()"
  },
  {
    "objectID": "Covid19.html#seaborn-library",
    "href": "Covid19.html#seaborn-library",
    "title": "1  Covid19 data analysis",
    "section": "1.3 Seaborn library",
    "text": "1.3 Seaborn library\n\nimport seaborn as sns\n\n\nnew_df = df1.loc[df1['location'].isin([\"India\",\"United States\"])]\nnew_df\n\n\n\n\n\n  \n    \n      \n      iso_code\n      continent\n      location\n      date\n      total_cases\n      new_cases\n      new_cases_smoothed\n      total_deaths\n      new_deaths\n      new_deaths_smoothed\n      ...\n      female_smokers\n      male_smokers\n      handwashing_facilities\n      hospital_beds_per_thousand\n      life_expectancy\n      human_development_index\n      excess_mortality_cumulative_absolute\n      excess_mortality_cumulative\n      excess_mortality\n      excess_mortality_cumulative_per_million\n    \n  \n  \n    \n      88211\n      IND\n      Asia\n      India\n      2020-01-30\n      1.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      88212\n      IND\n      Asia\n      India\n      2020-01-31\n      1.0\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      88213\n      IND\n      Asia\n      India\n      2020-02-01\n      1.0\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      88214\n      IND\n      Asia\n      India\n      2020-02-02\n      2.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      88215\n      IND\n      Asia\n      India\n      2020-02-03\n      3.0\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      1.9\n      20.6\n      59.55\n      0.53\n      69.66\n      0.645\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      203103\n      USA\n      North America\n      United States\n      2022-09-02\n      94733881.0\n      82785.0\n      78205.571\n      1047482.0\n      472.0\n      501.571\n      ...\n      19.1\n      24.6\n      NaN\n      2.77\n      78.86\n      0.926\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      203104\n      USA\n      North America\n      United States\n      2022-09-03\n      94743672.0\n      9791.0\n      77982.714\n      1047504.0\n      22.0\n      500.429\n      ...\n      19.1\n      24.6\n      NaN\n      2.77\n      78.86\n      0.926\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      203105\n      USA\n      North America\n      United States\n      2022-09-04\n      94749783.0\n      6111.0\n      77879.571\n      1047505.0\n      1.0\n      500.286\n      ...\n      19.1\n      24.6\n      NaN\n      2.77\n      78.86\n      0.926\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      203106\n      USA\n      North America\n      United States\n      2022-09-05\n      94769820.0\n      20037.0\n      66083.857\n      1047576.0\n      71.0\n      444.714\n      ...\n      19.1\n      24.6\n      NaN\n      2.77\n      78.86\n      0.926\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      203107\n      USA\n      North America\n      United States\n      2022-09-06\n      94898863.0\n      129043.0\n      74251.286\n      1048201.0\n      625.0\n      484.571\n      ...\n      19.1\n      24.6\n      NaN\n      2.77\n      78.86\n      0.926\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n1910 rows × 67 columns\n\n\n\n\nsns.jointplot(data=new_df,\\\n              x=\"new_cases\",y=\"new_deaths\", hue=\"location\")\n\n<seaborn.axisgrid.JointGrid at 0x23a3cb50d00>\n\n\n\n\n\n\nsns.jointplot(data=new_df, x=\"total_cases\",y=\"total_vaccinations\", hue=\"location\")\n\n<seaborn.axisgrid.JointGrid at 0x23a3cc41430>\n\n\n\n\n\n\nsns.pairplot(new_df[[\"location\",\"total_cases\",\"total_deaths\",\"total_vaccinations\"]],hue=\"location\")\n\n<seaborn.axisgrid.PairGrid at 0x23a3cdc6d00>"
  },
  {
    "objectID": "Interactive_visual.html",
    "href": "Interactive_visual.html",
    "title": "2  Interactive data visualization",
    "section": "",
    "text": "Ipython widgets are useful in adding interactivity in Jupyter notebook and the rendered HTML pages. These widgets are particularly helpful in creating interactive graphs and to enable customized rendering of dataframes.\nToogle widget"
  },
  {
    "objectID": "Interactive_visual.html#converting-notebooks-to-html",
    "href": "Interactive_visual.html#converting-notebooks-to-html",
    "title": "2  Interactive data visualization",
    "section": "2.1 Converting notebooks to html",
    "text": "2.1 Converting notebooks to html\nThe interactive visualizations within the notebooks can be converted to static webpages in the html format. The nbconvert package facilitates converting Jupyter notebooks to html. It can be installed via pip install nbconvert. Once installed, the following command can be used to convert a notebook test.ipynb to the html format.\njupyter nbconvert --to html test.ipynb\nThe resulting html file can be uploaded on any of the web hosting services such as Google Sites."
  },
  {
    "objectID": "Interactive_visual.html#bokeh-library",
    "href": "Interactive_visual.html#bokeh-library",
    "title": "2  Interactive data visualization",
    "section": "2.2 Bokeh library",
    "text": "2.2 Bokeh library\nInstallation - pip install bokeh\n\nimport pandas as pd\nfrom bokeh.plotting import figure, show\n\n\n## for showing graphs inside Jupyter notebook \nfrom bokeh.resources import INLINE\nimport bokeh.io\nbokeh.io.output_notebook(INLINE)\n\n\n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\nx = range(1,6)\ny = [a**2 for a in x]\nz = [b**3 for b in x]\np = figure(title=\"Line plot\", x_axis_label='Number', y_axis_label='Value',\\\n          width=500, height=250)\np.line(x, y, legend_label=\"Square\", color=\"teal\", line_width=2)\np.line(x, z, legend_label=\"Cube\", color=\"orange\", line_width=2) \nshow(p)\n\n\n  \n\n\n\n\n\n\ndf1 = pd.read_csv(\"owid-covid-data.csv\")\n\n\ndf1['date'] = pd.to_datetime(df1['date'])\np = figure(width=600, height=250,x_axis_type='datetime')\np.line(source=df1[df1[\"location\"]==\"India\"], x=\"date\", y=\"total_cases\", \\\n       color=\"indigo\", line_width=2)\np.line(source=df1[df1[\"location\"]==\"United States\"] ,x=\"date\", \\\n       y=\"total_cases\", color=\"magenta\", line_width=2)\nshow(p)"
  },
  {
    "objectID": "kmeans.html",
    "href": "kmeans.html",
    "title": "3  Unsupervised learning",
    "section": "",
    "text": "When working a dataset having dependent and independent variables but no class labels then we can train a machine learning model using unsupervised learning algorithms. Clustering of data is an example of unsupervised learning because here we group the data points without having any specific name for each group. The sklearn library has various algorithms for unsupervised learning. Here we’ll walk through the kmeans clustering algorithm using the iris dataset which contains data for four different parameters from three species of iris plant.\nYou can either download the iris dataset as a csv file and read it in a dataframe using read_csv function or directly load the dataset using the URL to create a new dataframe using the same function. We’ll load the iris data using the second option. In this data there are no headers i.e. there are no column names, so we’ll also create a list having the appropriate column names and pass is as an argument to the read_csv function."
  },
  {
    "objectID": "kmeans.html#data-pre-processing",
    "href": "kmeans.html#data-pre-processing",
    "title": "3  Unsupervised learning",
    "section": "3.1 Data pre-processing",
    "text": "3.1 Data pre-processing\nCreate x and y matrices having the observations and labels respectively. The x matrix would comprise of only the values of all the features for all the samples. This would be used for training the machine learning model. The y matrix would have the labels correponding to the samples in the x matrix. The y matrix is used in supervised classification. The x matrix is a 2d array with shape n_samples by n_features while the y matrix is a one dimensional array.\n\nx = iris.loc[:,iris.columns[0:4]].values\ny = iris.loc[:,\"Class\"].values\nprint(x.shape)\nprint(y.shape)\n\n(150, 4)\n(150,)\n\n\n\n## To get classes as int\ny_int = pd.get_dummies(y).values.argmax(1)\ny_int\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
  },
  {
    "objectID": "kmeans.html#model-building",
    "href": "kmeans.html#model-building",
    "title": "3  Unsupervised learning",
    "section": "3.2 Model building",
    "text": "3.2 Model building\nNext, a machine learning model would be built using the kmeans algorithm to predict the labels of the data i.e. the x matrix. This is achieved by instantiating a KMeans object with required arguments. The n_cluster keyword argument specifies the number of clusters that we want. In Kmeans jargon this number specifies the number of centroid to generate. the default value for this is 8. For our data we’ll set its value to 3 because we know that the data is from three species. In case we don’t know how many clusters to expect, we can figure that out using the elbow method which we’ll cover in a while. The init argument is for initializing the positioning of the centroids. We can set the initial position based on an empirical probability distribution of the data points using the k-means++ or we can also specify the exact location for initial positioning of centroids. The n_init and max_iter arguments refer to number of independent runs of the kmeans algorirhtm and maximum number of iterations of the algorithm in each run, respectively. Setting the random_state argument to an int ensures the reproducibilty of the results.\nOnce we have instantiate a KMeans object, we can use the fit_predict function to first fit the data to the model, and then predict the label of the given data. It returns an array of labels corresponding to the each data point (sample).\n\nkmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x)\ny_kmeans\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n       2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2,\n       2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0])\n\n\nThe fit_predict function returns the predicted index of the cluster corresponding to all the data points. Note that this function is effectively a combination of two functions – fit and predict. Once the data has been fitted to the KMeans object, different output parameters for the model can be accessed through its attributes such as - cluster_centers_ to get the coordinates of all the centroids - labels_ to get the predicted labels (cluster indices) for the data points.\nThe predict function can be used to predict the label for a given data point.\n\n#get centers for each of the clusters\nkmeans.cluster_centers_\n\narray([[5.9016129 , 2.7483871 , 4.39354839, 1.43387097],\n       [5.006     , 3.418     , 1.464     , 0.244     ],\n       [6.85      , 3.07368421, 5.74210526, 2.07105263]])\n\n\n\n#predicting the cluster of an unknown observation\nkmeans.predict([[5.1,3.,1.4,0.2]])\n\narray([1])\n\n\n\nCompare the predicted labels with the original labels.\nWhen the data is fitted to the KMeans model, the predicted labels could be in any order i.e. the cluster are numbered randomly. Therefore, direct mapping of the predicted labels and original labels could result in misleading interpretations of the prediction accuracy. So, to compare the predicted cluster with the original labels, we need to take into account this characteristic of the clustering function. The adjusted_rand_score function compares the members of different cluster in context of the cluster labels.\n\nfrom sklearn.metrics import adjusted_rand_score\nadjusted_rand_score(y, kmeans.labels_)\n\n0.7302382722834697\n\n\nTo perform the visual assessment of the predictions, let’s plot the clusters using first two features. This way it would be convenient to see the differences in the original and predicted labels in a two-dimensional graph. We’ll first create a copy of the iris dataframe and add a new column which would hold the predicted labels. Subsequently, the scatterplot function from the seaborn library for plotting and the coloring of the data points would be based on the class labels (original and predicted separately).\n\nimport seaborn as sns\n\n\niris_predicted = iris.copy()\niris_predicted[\"Predicted_Class\"] = y_kmeans\niris_predicted.head()\n\n\n\n\n\n  \n    \n      \n      Sepal_Length\n      Sepal_Width\n      Petal_Length\n      Petal_Width\n      Class\n      Predicted_Class\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      Iris-setosa\n      1\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      Iris-setosa\n      1\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      Iris-setosa\n      1\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      Iris-setosa\n      1\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      Iris-setosa\n      1\n    \n  \n\n\n\n\n\nfig, ax = plt.subplots(1,2, figsize=(10,5), sharey=True)\n\nax[0].set_title(\"Original Labels\")\nsns.scatterplot(data=iris_predicted, x=\"Sepal_Length\", y=\"Sepal_Width\", ax=ax[0], hue=\"Class\")\nax[0].set_xlabel(\"\")\nax[0].set_ylabel(\"\")\n\nax[1].set_title(\"Predicted Labels\")\nsns.scatterplot(data=iris_predicted, x=\"Sepal_Length\", y=\"Sepal_Width\", ax=ax[1], hue=\"Predicted_Class\")\nax[1].set_xlabel(\"\")\n\nfig.supxlabel(\"Sepal Length\")\nfig.supylabel(\"Sepal Width\")\nplt.tight_layout()\n\n\n\n\nSimilarly, we can plot pairwise plot for all the features in the dataframe using sns.pairplot function.\n\np1 = sns.pairplot(data=iris, hue=\"Class\", corner=True)\n\n\n\n\n\np2 = sns.pairplot(data=iris_predicted, hue=\"Predicted_Class\", corner=True)"
  },
  {
    "objectID": "kmeans.html#finding-optimal-number-of-clusters",
    "href": "kmeans.html#finding-optimal-number-of-clusters",
    "title": "3  Unsupervised learning",
    "section": "3.3 Finding optimal number of clusters",
    "text": "3.3 Finding optimal number of clusters\nOne of the most important hyperparameter for a KMeans object is the n_clusters which stores the value for the number of clusters to create. We can empirically determine an suitable value for this hyperparameter by using the elbow method. In this, we’ll iteratively fit the data to the kMeans objects instantiated with different number values for n_clusters. Algorithmically speaking, the KMeans algorithm select centroids that minimize the inertia (or within-cluster sum-of-squares). In other words, the coordinates for the centroids are such that the sum of distances between each data point (within a cluster) and the cluster center is minimum. Now, upon plotting the inertia values vs the number of clusters, the graph tends to plateau around the optimal number of clusters; which appears like an elbow. As the number of clusters increase there is a decrease in inertia, however, after a certain number of clusters the decrease in inertia is much less compared to what was observed initially. Mathematically, the K-means algorithm operates to minimize the inertia as follows (reference):\n\\[\n\\sum_{i=0}^{n}\\underset{\\mu_{j} \\in C}{\\mathrm{min}} (||x_{i} - \\mu_{j}||^{2})\n\\]\nwhere, \\(\\mu_{j}\\) is a cluster center and \\(C\\) is the set of all clusters. \\(x_{i}\\) denotes a data point in a given cluster.\n\nwcss = [] #within cluster sum of squares\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, \\\n                    n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia') \nplt.show()"
  },
  {
    "objectID": "ML_supervised.html",
    "href": "ML_supervised.html",
    "title": "4  Supervised learning",
    "section": "",
    "text": "Classification for proteins based on sequence information is an important tool for their functional annotation. Supervised machine learning algorithms are a lucrative option to develop classification models. Starting with a set to protein sequences that belong to a particular class (positive dataset) and a set of protein sequences that do not belong to that particualar class (negative dataset), machines learning model is built to classify unknown protein sequences. The model trains itself based on certain set of “features” that represent the different sequences in the two datasets."
  },
  {
    "objectID": "ML_supervised.html#datasets",
    "href": "ML_supervised.html#datasets",
    "title": "4  Supervised learning",
    "section": "4.1 Datasets",
    "text": "4.1 Datasets\nThe first step for any supervised machine learning execise is to prepare a dataset having positive and negative cases. In our case we’ll need a set of protein sequences that would represent positive and negative classes. We know that proteins are polymers made of 20 different amino acids. Proteins have been classified into different families based on their sequence similarity. For practice, we’ll use some of the dataset that available in the literature. Below are two such datasets correponding to two different binary classification model building. - Positive and negative datasets corresponding to one of the protein families are available here. - A dataset comprising of UniProt IDs of postive and negative sequences is available here."
  },
  {
    "objectID": "ML_supervised.html#feature-extraction",
    "href": "ML_supervised.html#feature-extraction",
    "title": "4  Supervised learning",
    "section": "4.2 Feature extraction",
    "text": "4.2 Feature extraction\nThe next and one of the most important step in machine learning is feature extraction. For classifcation of protein sequences we cannot input the amino acid sequence directly to the classifier. So, we need to transform our data such that each sequence can be represent as a feature of constant length. Amino acid composition is one of the feature that can be used in this senario. Amino Acid Composition refers to frequency of each amino acid within a protein sequence. E.g. if a protein has a sequence ‘MSAARQTTRKAE’ it’s amino acid composition can be represented as a vector of length 20:\n‘A’:3,‘C’:0,‘D’:0,‘E’:1,‘F’:0,‘G’:0,‘H’:0,‘I’:0,‘K’:1,‘L’:0,‘M’:1,‘N’:0,‘P’:0,‘Q’:1,‘R’:1,‘S’:1,‘T’:2,‘V’:0,‘W’:0,‘Y’:0\nIn this way any protein sequence can be represented as a feature vector of fixed length. This is important because the protein sequences, even within the same class, can have different number of amino acids. To start with feature extraction and model building, we’ll import the required libraries.\n\nimport scipy\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom Bio import SeqIO\nfrom Bio.SeqUtils.ProtParam import ProteinAnalysis\nfrom sklearn import model_selection\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\nFirst, we’ll use SeqIO.parse function to read the sequence files and make a dictionary of all the sequences using the to_dict function. This dictionary would have fasta header as the keys of the dictionary and the sequence object as the corresponding values.\n\npositive_dict = SeqIO.to_dict(SeqIO.parse(\"positive-aars.fasta\", \"fasta\"))\nnegative_dict = SeqIO.to_dict(SeqIO.parse(\"negative-aars.fasta\", \"fasta\"))\n\n## Amino acid composition calculation##\n#c1 = ProteinAnalysis(\"AAAASTRRRTRRAWEQWERQW\").count_amino_acids()\ndf1 = pd.DataFrame()\nfor keys,values in positive_dict.items():\n    df1 = pd.concat([df1, pd.Series(ProteinAnalysis(str(values.seq)).get_amino_acids_percent(),name='1').to_frame().T])\nfor keys,values in negative_dict.items():\n    df1 = pd.concat([df1, pd.Series(ProteinAnalysis(str(values.seq)).get_amino_acids_percent(),name='-1').to_frame().T])\n\nprint(\"Number of positive samples: \", len(positive_dict))\nprint(\"Number of negative samples: \", len(negative_dict))\n\ndf1\n\nNumber of positive samples:  117\nNumber of negative samples:  117\n\n\n\n\n\n\n  \n    \n      \n      A\n      C\n      D\n      E\n      F\n      G\n      H\n      I\n      K\n      L\n      M\n      N\n      P\n      Q\n      R\n      S\n      T\n      V\n      W\n      Y\n    \n  \n  \n    \n      1\n      0.035928\n      0.007984\n      0.041916\n      0.131737\n      0.055888\n      0.049900\n      0.017964\n      0.081836\n      0.119760\n      0.107784\n      0.015968\n      0.037924\n      0.037924\n      0.011976\n      0.037924\n      0.055888\n      0.049900\n      0.057884\n      0.013972\n      0.029940\n    \n    \n      1\n      0.076233\n      0.004484\n      0.049327\n      0.065770\n      0.029895\n      0.082212\n      0.020927\n      0.055306\n      0.082212\n      0.074738\n      0.040359\n      0.037369\n      0.043348\n      0.056801\n      0.058296\n      0.055306\n      0.061286\n      0.053812\n      0.017937\n      0.034380\n    \n    \n      1\n      0.093333\n      0.009524\n      0.068571\n      0.080000\n      0.026667\n      0.120000\n      0.024762\n      0.059048\n      0.055238\n      0.095238\n      0.028571\n      0.015238\n      0.041905\n      0.019048\n      0.074286\n      0.060952\n      0.020952\n      0.053333\n      0.009524\n      0.043810\n    \n    \n      1\n      0.079470\n      0.036424\n      0.052980\n      0.067881\n      0.046358\n      0.072848\n      0.023179\n      0.046358\n      0.021523\n      0.117550\n      0.019868\n      0.016556\n      0.046358\n      0.039735\n      0.081126\n      0.072848\n      0.049669\n      0.072848\n      0.008278\n      0.028146\n    \n    \n      1\n      0.084158\n      0.003300\n      0.046205\n      0.094059\n      0.034653\n      0.077558\n      0.018152\n      0.061056\n      0.089109\n      0.099010\n      0.036304\n      0.057756\n      0.031353\n      0.029703\n      0.026403\n      0.047855\n      0.042904\n      0.061056\n      0.011551\n      0.047855\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      -1\n      0.087221\n      0.004057\n      0.050710\n      0.060852\n      0.034483\n      0.068966\n      0.020284\n      0.056795\n      0.070994\n      0.089249\n      0.020284\n      0.036511\n      0.050710\n      0.042596\n      0.054767\n      0.079108\n      0.052738\n      0.075051\n      0.004057\n      0.040568\n    \n    \n      -1\n      0.067901\n      0.009259\n      0.049383\n      0.030864\n      0.027778\n      0.055556\n      0.024691\n      0.037037\n      0.030864\n      0.080247\n      0.015432\n      0.040123\n      0.111111\n      0.080247\n      0.104938\n      0.101852\n      0.058642\n      0.030864\n      0.018519\n      0.024691\n    \n    \n      -1\n      0.053521\n      0.010329\n      0.056338\n      0.101408\n      0.037559\n      0.037559\n      0.014085\n      0.059155\n      0.082629\n      0.110798\n      0.025352\n      0.059155\n      0.029108\n      0.047887\n      0.061033\n      0.078873\n      0.048826\n      0.046948\n      0.005634\n      0.033803\n    \n    \n      -1\n      0.038462\n      0.022869\n      0.070686\n      0.062370\n      0.062370\n      0.024948\n      0.014553\n      0.066528\n      0.060291\n      0.121622\n      0.016632\n      0.069647\n      0.027027\n      0.040541\n      0.050936\n      0.096674\n      0.061331\n      0.058212\n      0.009356\n      0.024948\n    \n    \n      -1\n      0.042791\n      0.014884\n      0.055814\n      0.071628\n      0.058605\n      0.032558\n      0.020465\n      0.053953\n      0.071628\n      0.110698\n      0.010233\n      0.054884\n      0.044651\n      0.042791\n      0.059535\n      0.102326\n      0.040930\n      0.054884\n      0.013023\n      0.043721\n    \n  \n\n234 rows × 20 columns\n\n\n\n\n#correlation between different features \nsns.heatmap(df1.corr())\n\n<AxesSubplot:>"
  },
  {
    "objectID": "ML_supervised.html#model-building",
    "href": "ML_supervised.html#model-building",
    "title": "4  Supervised learning",
    "section": "4.3 Model building",
    "text": "4.3 Model building\nCreate data and label matrices.\n\ndf1_matrix = df1.iloc[:,range(20)].values ## X matrix\nlabels = df1.index.values ## Y matrix\n\nCreate training and testing datasets, build the model and check prediction accuracy.\n\n# Split-out validation dataset\"\nvalidation_size = 0.20 # 20% data for testing\nseed = None # change to int for reproducibility\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(df1_matrix, labels, \\\n                                                                               test_size=validation_size, \\\n                                                                                random_state=seed)\n\nnew_clf = SVC()\nmodel1 = new_clf.fit(X_train, Y_train)\nY_predicted = model1.predict(X_validation)\nscore1 = accuracy_score(Y_predicted, Y_validation)\nprint(score1)\n\n0.8085106382978723\n\n\n\ndf1_matrix.shape\n\n(234, 20)"
  },
  {
    "objectID": "ML_supervised.html#cross-validation",
    "href": "ML_supervised.html#cross-validation",
    "title": "4  Supervised learning",
    "section": "4.4 Cross-validation",
    "text": "4.4 Cross-validation\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\n# Test options and evaluation metric\nscoring = 'accuracy'\n\nkfold = model_selection.KFold(n_splits=5)\ncv_results = model_selection.cross_val_score(SVC(), df1_matrix, labels, cv=kfold, scoring=scoring)\n\nprint(cv_results)\nprint(cv_results.mean(), cv_results.std())\n\n[0.38297872 0.34042553 0.65957447 0.34042553 0.30434783]\n0.40555041628122107 0.12943119067958725\n\n\n\ncv_results\n\narray([0.38297872, 0.34042553, 0.65957447, 0.34042553, 0.30434783])\n\n\n\nSVC().get_params()\n\n{'C': 1.0,\n 'break_ties': False,\n 'cache_size': 200,\n 'class_weight': None,\n 'coef0': 0.0,\n 'decision_function_shape': 'ovr',\n 'degree': 3,\n 'gamma': 'scale',\n 'kernel': 'rbf',\n 'max_iter': -1,\n 'probability': False,\n 'random_state': None,\n 'shrinking': True,\n 'tol': 0.001,\n 'verbose': False}"
  },
  {
    "objectID": "ML_supervised.html#hyperparameters",
    "href": "ML_supervised.html#hyperparameters",
    "title": "4  Supervised learning",
    "section": "4.5 Hyperparameters",
    "text": "4.5 Hyperparameters\n\nclf_poly = SVC(kernel='poly', degree=3)\nclf_rbf = SVC(kernel='rbf', C=10)\n\nkfold = model_selection.KFold(n_splits=5)\ncv_SVM_poly_results = model_selection.cross_val_score(clf_poly, df1_matrix, labels, cv=kfold, scoring=scoring)\ncv_SVM_rbf_results = model_selection.cross_val_score(clf_rbf, df1_matrix, labels, cv=kfold, scoring=scoring)\nprint(cv_SVM_rbf_results)\nprint (cv_SVM_poly_results.mean(), cv_SVM_rbf_results.mean())\n\n[0.70212766 0.65957447 0.78723404 0.5106383  0.65217391]\n0.6281221091581869 0.6623496762257168"
  },
  {
    "objectID": "ML_supervised.html#grid-search",
    "href": "ML_supervised.html#grid-search",
    "title": "4  Supervised learning",
    "section": "4.6 Grid Search",
    "text": "4.6 Grid Search\n\nfrom sklearn.model_selection import GridSearchCV\ntuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000, 10000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000, 10000]},\n                    {'kernel':['poly'], 'C': [1, 10, 100, 1000, 10000],\n                     'degree': range(10)}]\nclf_grid = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='accuracy')\nclf_grid.fit(df1_matrix,labels)\n\nGridSearchCV(cv=5, estimator=SVC(),\n             param_grid=[{'C': [1, 10, 100, 1000, 10000],\n                          'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n                         {'C': [1, 10, 100, 1000, 10000], 'kernel': ['linear']},\n                         {'C': [1, 10, 100, 1000, 10000],\n                          'degree': range(0, 10), 'kernel': ['poly']}],\n             scoring='accuracy')\n\n\n\nprint(clf_grid.best_params_)\n#print(clf_grid.cv_results_)\n\n{'C': 1, 'degree': 3, 'kernel': 'poly'}\n\n\nBuild a SVM model using above hyperparameters and check the prediction accuracy.\n\nclf_poly = SVC(kernel='poly',C=1, degree=3)\n\nkfold = model_selection.KFold(n_splits=5)\ncv_SVM_results = model_selection.cross_val_score(clf_poly, df1_matrix, labels, cv=kfold, scoring=scoring)\nprint(cv_SVM_results)\nprint (cv_SVM_results.mean(),cv_SVM_results.std())\n\n[0.70212766 0.65957447 0.76595745 0.40425532 0.60869565]\n0.6281221091581869 0.12325451681757636"
  },
  {
    "objectID": "ML_supervised.html#confusion-matrix",
    "href": "ML_supervised.html#confusion-matrix",
    "title": "4  Supervised learning",
    "section": "4.7 Confusion matrix",
    "text": "4.7 Confusion matrix\n\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix\n\n\nclf_new = SVC(kernel='poly',degree=3, probability=True)\n\n## Fit model to the data ##\nclf_new.fit(df1_matrix,labels)\nprint(Counter(labels))\n\n## Predict labels for the original data ##\nclf_new_predict = clf_new.predict(df1_matrix)\nprint(Counter(clf_new_predict))\n\nCounter({'1': 117, '-1': 117})\nCounter({'-1': 121, '1': 113})\n\n\n\ny_proba = clf_new.predict_proba(df1_matrix)\ny_proba[0]\n\narray([0.28550858, 0.71449142])\n\n\n\ntype(clf_new_predict)\n\nnumpy.ndarray\n\n\n\ncm = confusion_matrix(labels,clf_new_predict)\nprint(cm)\n\nfig,ax= plt.subplots()\nsns.heatmap(cm, square=True, annot=True, fmt='g', cbar=False, ax=ax)\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.xaxis.set_ticklabels(['positive', 'negative'])\nax.yaxis.set_ticklabels(['positive', 'negative'])\nplt.show()\n\n[[108   9]\n [ 13 104]]"
  },
  {
    "objectID": "ML_supervised.html#roc-curve",
    "href": "ML_supervised.html#roc-curve",
    "title": "4  Supervised learning",
    "section": "4.8 ROC Curve",
    "text": "4.8 ROC Curve\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import RocCurveDisplay\n\nfpr, tpr, _ = roc_curve(labels.astype('int'), y_proba[:, 1])#clf_new_predict.astype('int'))\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\nroc_display.figure_.set_size_inches(5,5)\nplt.plot([0, 1], [0, 1], color = 'grey', linestyle=\"dashed\")\nplt.show()\n# Plots the ROC curve using the sklearn methods - Good plot\n#plot_sklearn_roc_curve(y_test, y_proba[:, 1])\n\n\n\n\n\nExercise\nExtract another feature - Di-Peptide Composition (DPC)\nFor each sequence calculate the frequency of pairwaise occurrence of amino acids. The length of the feature vector for each sequence would be 400 (20 x 20). Construct a classification model using DPC as a feature and compare the results with classification using amino acid composition.\n\nimport itertools\naa_list = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n\n\n## Create a series of dipeptides\ndpc_series = pd.Series(name='1',dtype=float)\nfor x in itertools.product(aa_list,aa_list):\n    dpc_series[''.join([''.join(x)])] = 0\n\n\ndf_dpc = pd.DataFrame([])\nfor keys,values in positive_dict.items():\n    dpc_series_copy = dpc_series.copy()\n#    print (values.seq)\n    dpc_seq = [str(values.seq[i:i+2]) for i in range(len(values.seq))]\n    del dpc_seq[-1]\n    for x in dpc_seq:\n        dpc_series_copy[x] += 1\n    dpc_series_copy /= len(values.seq)\n    dpc_series_copy *= 100\n    df_dpc = df_dpc.append(dpc_series_copy)\n#dpc_series_copy\n\n\ndf_dpc\n\n\n\n\n\n  \n    \n      \n      AA\n      AC\n      AD\n      AE\n      AF\n      AG\n      AH\n      AI\n      AK\n      AL\n      ...\n      YM\n      YN\n      YP\n      YQ\n      YR\n      YS\n      YT\n      YV\n      YW\n      YY\n    \n  \n  \n    \n      1\n      0.399202\n      0.000000\n      0.000000\n      0.399202\n      0.399202\n      0.399202\n      0.199601\n      0.199601\n      0.598802\n      0.199601\n      ...\n      0.199601\n      0.000000\n      0.598802\n      0.199601\n      0.000000\n      0.199601\n      0.000000\n      0.000000\n      0.0\n      0.000000\n    \n    \n      1\n      1.345291\n      0.000000\n      0.149477\n      0.747384\n      0.149477\n      0.597907\n      0.149477\n      0.298954\n      0.896861\n      0.448430\n      ...\n      0.000000\n      0.000000\n      0.298954\n      0.448430\n      0.000000\n      0.000000\n      0.298954\n      0.597907\n      0.0\n      0.000000\n    \n    \n      1\n      0.571429\n      0.000000\n      0.761905\n      0.761905\n      0.380952\n      1.142857\n      0.190476\n      0.380952\n      0.571429\n      1.142857\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.761905\n      0.190476\n      0.190476\n      0.190476\n      0.0\n      0.000000\n    \n    \n      1\n      0.331126\n      0.165563\n      0.331126\n      0.496689\n      0.662252\n      0.662252\n      0.331126\n      0.000000\n      0.165563\n      0.496689\n      ...\n      0.000000\n      0.165563\n      0.165563\n      0.331126\n      0.827815\n      0.331126\n      0.000000\n      0.165563\n      0.0\n      0.000000\n    \n    \n      1\n      0.495050\n      0.000000\n      0.660066\n      0.990099\n      0.165017\n      0.660066\n      0.000000\n      0.165017\n      0.825083\n      0.825083\n      ...\n      0.165017\n      0.495050\n      0.165017\n      0.165017\n      0.165017\n      0.165017\n      0.165017\n      0.330033\n      0.0\n      0.330033\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1\n      0.790514\n      0.000000\n      0.592885\n      0.395257\n      0.592885\n      0.592885\n      0.197628\n      0.197628\n      1.185771\n      0.395257\n      ...\n      0.000000\n      0.000000\n      0.197628\n      0.000000\n      0.395257\n      0.197628\n      0.000000\n      0.000000\n      0.0\n      0.000000\n    \n    \n      1\n      0.401606\n      0.000000\n      0.401606\n      0.200803\n      1.004016\n      0.200803\n      0.401606\n      0.000000\n      0.803213\n      0.200803\n      ...\n      0.000000\n      0.000000\n      0.000000\n      0.401606\n      0.200803\n      0.602410\n      0.200803\n      0.803213\n      0.0\n      0.401606\n    \n    \n      1\n      2.923977\n      0.000000\n      1.754386\n      1.949318\n      1.169591\n      0.779727\n      0.194932\n      0.389864\n      0.000000\n      1.559454\n      ...\n      0.000000\n      0.000000\n      0.194932\n      0.000000\n      0.584795\n      0.000000\n      0.000000\n      0.000000\n      0.0\n      0.000000\n    \n    \n      1\n      0.000000\n      0.000000\n      0.114416\n      0.114416\n      0.114416\n      0.114416\n      0.114416\n      0.114416\n      0.343249\n      0.114416\n      ...\n      0.000000\n      0.228833\n      0.228833\n      0.000000\n      0.228833\n      0.228833\n      0.228833\n      0.228833\n      0.0\n      0.228833\n    \n    \n      1\n      0.000000\n      0.142653\n      0.285307\n      0.000000\n      0.570613\n      0.000000\n      0.285307\n      0.855920\n      0.570613\n      1.711840\n      ...\n      0.000000\n      0.000000\n      0.142653\n      0.427960\n      0.000000\n      0.000000\n      0.142653\n      0.000000\n      0.0\n      0.285307\n    \n  \n\n117 rows × 400 columns"
  },
  {
    "objectID": "PCA_tutorial.html",
    "href": "PCA_tutorial.html",
    "title": "5  Principal Component Analysis",
    "section": "",
    "text": "PCA is an unsupervised learning algorithm which is widely used to reduce the dimensions of large datasets. It can also be used for feature extraction such that we can use a limited number of features. PCA helps to identify feature(s) that contributes maximally to the variance in the data.\nPCA transforms the original data to a lower dimensional space while minimizing the information loss.\nHere we’ll use a dataset from this manuscript to receate some of the analysis presented in the manuscript. This manuscript presents analysis of 8 phenotypic traits in 169 japonica rice varieties. The data used here is available in the supplementary material of the manuscript download. Save this excel file in csv format.\nRead data in csv format having parameters for different aspect of the plant over two years. Columns 3 to 10 and 11 to 18 for 2014 and 2015, respectively. The first column is “Line number” so we’ll use that as index for our dataframe and second column is plant variety. There is no header row in the csv file. Let’s read the data and preprocess it."
  },
  {
    "objectID": "PCA_tutorial.html#data-pre-processing",
    "href": "PCA_tutorial.html#data-pre-processing",
    "title": "5  Principal Component Analysis",
    "section": "5.1 Data pre-processing",
    "text": "5.1 Data pre-processing\nNext, we’ll extract the data for the two years separately and merge them vertically in one dataframe. This reformating would be useful for subsequent analysis. We’ll also add column header to our dataframe. These headers would be the name of different parameters.\n\ndf_2015 = df1[[1,10,11,12,13,14,15,16,17]]\ndf_2015.columns=[\"Variety\",\"Days-to-heading\", \"Culm length (cm)\",\"Panicle No.\", \"Panicle length (cm)\", \"Rachis length (cm)\", \"Primary branch No.\", \"Sec. branch No.\", \"Spikelet No.\"]\ndf_2015[\"Year\"]=2015\ndf_2015.head()\n\n\n\n\n\n  \n    \n      \n      Variety\n      Days-to-heading\n      Culm length (cm)\n      Panicle No.\n      Panicle length (cm)\n      Rachis length (cm)\n      Primary branch No.\n      Sec. branch No.\n      Spikelet No.\n      Year\n    \n    \n      0\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Line 1\n      GORIKI\n      101\n      117.1\n      8.3\n      25.4\n      21.0\n      13.7\n      46.7\n      221.7\n      2015\n    \n    \n      Line 2\n      HINOMARU\n      87\n      99.8\n      7.7\n      23.6\n      17.3\n      12.0\n      41.8\n      213.0\n      2015\n    \n    \n      Line 3\n      MANGOKU\n      91\n      106.0\n      10.7\n      23.6\n      18.3\n      14.7\n      32.0\n      177.7\n      2015\n    \n    \n      Line 4\n      KAIRYO SHINKOU\n      87\n      86.3\n      7.3\n      25.3\n      19.6\n      12.7\n      35.7\n      182.0\n      2015\n    \n    \n      Line 5\n      SENICHI\n      98\n      102.0\n      14.0\n      26.0\n      20.0\n      11.7\n      26.7\n      140.3\n      2015\n    \n  \n\n\n\n\nRemove that columns from df1 that have been copied to df2 and add headers (same as that in df2) to df1. Also, add the year column to df1 such that df1 would now have data for only 2014.\n\ndf_2014 = df1.drop(columns=[10,11,12,13,14,15,16,17])\ndf_2014.columns=[\"Variety\",\"Days-to-heading\", \"Culm length (cm)\",\"Panicle No.\", \"Panicle length (cm)\", \"Rachis length (cm)\", \"Primary branch No.\", \"Sec. branch No.\", \"Spikelet No.\"]\ndf_2014[\"Year\"]=2014\ndf_2014.head()\n\n\n\n\n\n  \n    \n      \n      Variety\n      Days-to-heading\n      Culm length (cm)\n      Panicle No.\n      Panicle length (cm)\n      Rachis length (cm)\n      Primary branch No.\n      Sec. branch No.\n      Spikelet No.\n      Year\n    \n    \n      0\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Line 1\n      GORIKI\n      98\n      112.0\n      7.0\n      26.7\n      19.6\n      12.7\n      41.3\n      206.3\n      2014\n    \n    \n      Line 2\n      HINOMARU\n      87\n      95.7\n      9.0\n      23.4\n      18.6\n      12.7\n      39.0\n      199.0\n      2014\n    \n    \n      Line 3\n      MANGOKU\n      93\n      109.7\n      9.0\n      24.6\n      20.7\n      16.7\n      36.0\n      210.0\n      2014\n    \n    \n      Line 4\n      KAIRYO SHINKOU\n      89\n      82.6\n      9.0\n      22.8\n      16.1\n      9.7\n      26.3\n      139.7\n      2014\n    \n    \n      Line 5\n      SENICHI\n      98\n      103.1\n      11.0\n      23.1\n      17.1\n      10.3\n      24.3\n      136.0\n      2014\n    \n  \n\n\n\n\nNow we have two dataframes having plant parameters for the two years i.e. df1 and df2 corresponding to 2014 and 2015, respectively. Let’s combine these two dataframes vertically to get the full data as one dataframe.\n\ndf_final = pd.concat([df1,df2])\ndf_final\n\n\n\n\n\n  \n    \n      \n      Variety\n      Days-to-heading\n      Culm length (cm)\n      Panicle No.\n      Panicle length (cm)\n      Rachis length (cm)\n      Primary branch No.\n      Sec. branch No.\n      Spikelet No.\n      Year\n    \n    \n      0\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Line 1\n      GORIKI\n      98\n      112.0\n      7.0\n      26.7\n      19.6\n      12.7\n      41.3\n      206.3\n      2014\n    \n    \n      Line 2\n      HINOMARU\n      87\n      95.7\n      9.0\n      23.4\n      18.6\n      12.7\n      39.0\n      199.0\n      2014\n    \n    \n      Line 3\n      MANGOKU\n      93\n      109.7\n      9.0\n      24.6\n      20.7\n      16.7\n      36.0\n      210.0\n      2014\n    \n    \n      Line 4\n      KAIRYO SHINKOU\n      89\n      82.6\n      9.0\n      22.8\n      16.1\n      9.7\n      26.3\n      139.7\n      2014\n    \n    \n      Line 5\n      SENICHI\n      98\n      103.1\n      11.0\n      23.1\n      17.1\n      10.3\n      24.3\n      136.0\n      2014\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Line 165\n      HIMENOMOCHI\n      81\n      82.8\n      10.3\n      21.5\n      15.8\n      13.7\n      30.0\n      165.7\n      2015\n    \n    \n      Line 166\n      SHINSHUU\n      91\n      85.8\n      11.3\n      20.3\n      14.5\n      10.3\n      16.0\n      102.0\n      2015\n    \n    \n      Line 167\n      AICHI ASAHI_2***\n      107\n      88.3\n      10.7\n      24.8\n      18.6\n      10.0\n      25.7\n      141.0\n      2015\n    \n    \n      Line 168\n      RAIDEN\n      109\n      108.3\n      8.7\n      24.7\n      18.9\n      12.3\n      36.3\n      165.3\n      2015\n    \n    \n      Line 169\n      HOUMANSHINDEN INE\n      120\n      122.0\n      12.3\n      23.6\n      18.7\n      13.7\n      42.3\n      220.7\n      2015\n    \n  \n\n338 rows × 10 columns\n\n\n\n\ndf_temp = df_final.reset_index(drop=True)\nsns.pairplot(data=df_temp.iloc[:,range(1,10)], hue=\"Year\", palette=\"deep\")\n\n<seaborn.axisgrid.PairGrid at 0x1376039d0>"
  },
  {
    "objectID": "PCA_tutorial.html#model-building",
    "href": "PCA_tutorial.html#model-building",
    "title": "5  Principal Component Analysis",
    "section": "5.2 Model building",
    "text": "5.2 Model building\nPerform the PCA using the normalized feature vectors to generate five components. We’ll first create an object of the PCA class with five as the value for n_components argument. Subsequently we’ll use the normalized matrix to generate principle components and save the results to a dataframe. We’ll first do PCA of the 2015 data only.\n\n# Exclude the Variety column\ndf_2015.iloc[:,[1,2,3,4,5,6,7,8]]\n\n\n\n\n\n  \n    \n      \n      Days-to-heading\n      Culm length (cm)\n      Panicle No.\n      Panicle length (cm)\n      Rachis length (cm)\n      Primary branch No.\n      Sec. branch No.\n      Spikelet No.\n    \n    \n      0\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Line 1\n      101\n      117.1\n      8.3\n      25.4\n      21.0\n      13.7\n      46.7\n      221.7\n    \n    \n      Line 2\n      87\n      99.8\n      7.7\n      23.6\n      17.3\n      12.0\n      41.8\n      213.0\n    \n    \n      Line 3\n      91\n      106.0\n      10.7\n      23.6\n      18.3\n      14.7\n      32.0\n      177.7\n    \n    \n      Line 4\n      87\n      86.3\n      7.3\n      25.3\n      19.6\n      12.7\n      35.7\n      182.0\n    \n    \n      Line 5\n      98\n      102.0\n      14.0\n      26.0\n      20.0\n      11.7\n      26.7\n      140.3\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Line 165\n      81\n      82.8\n      10.3\n      21.5\n      15.8\n      13.7\n      30.0\n      165.7\n    \n    \n      Line 166\n      91\n      85.8\n      11.3\n      20.3\n      14.5\n      10.3\n      16.0\n      102.0\n    \n    \n      Line 167\n      107\n      88.3\n      10.7\n      24.8\n      18.6\n      10.0\n      25.7\n      141.0\n    \n    \n      Line 168\n      109\n      108.3\n      8.7\n      24.7\n      18.9\n      12.3\n      36.3\n      165.3\n    \n    \n      Line 169\n      120\n      122.0\n      12.3\n      23.6\n      18.7\n      13.7\n      42.3\n      220.7\n    \n  \n\n169 rows × 8 columns\n\n\n\nNormalization\n\npca_2015_standard = StandardScaler().fit_transform(df_2015.iloc[:,[1,2,3,4,5,6,7,8]])\n\nFit the data and create a dataframe of principle components\n\npca_2015_model = PCA(n_components=5) #number of principal components\npca_2015_fit = pca_2015_model.fit_transform(pca_2015_standard) \n\ndf_2015_pca = pd.DataFrame(data = pca_2015_fit, columns = ['pc1', 'pc2','pc3','pc4','pc5'])\n\nThe variance in the data as explained by each of the component can be check by the explained_variance_ratio_ of the pca object. The output is the variance associated with each of the principal components (five in this case). We can also calculate the total variance explained by the five components. As we see below the five components together account for 95.9% variance in the data.\n\nprint(pca_2015_model.explained_variance_ratio_)\nprint(pca_2015_model.explained_variance_ratio_.cumsum())\n\n[0.62283251 0.16410666 0.07756149 0.05617533 0.03833603]\n[0.62283251 0.78693917 0.86450066 0.92067599 0.95901202]\n\n\n\npca_2015_model.components_\n\narray([[-0.16505669, -0.36898558,  0.24285994, -0.3816588 , -0.408246  ,\n        -0.3591661 , -0.40835848, -0.41175211],\n       [-0.7266037 , -0.2441704 , -0.47751906, -0.21155821, -0.13636855,\n         0.2424033 ,  0.15019558,  0.19932913],\n       [-0.22625375,  0.20730652,  0.74838672, -0.30732605, -0.09178569,\n         0.41958235,  0.14012806,  0.21723589],\n       [-0.41965494, -0.32972906,  0.31061083,  0.53659936,  0.50113939,\n         0.04226267, -0.23352974, -0.15260259],\n       [ 0.30823769, -0.18116406, -0.18564851, -0.1950358 ,  0.12892964,\n         0.73701089, -0.43027916, -0.23391603]])\n\n\n\ndf_2015_pca.head()\n\n\n\n\n\n  \n    \n      \n      pc1\n      pc2\n      pc3\n      pc4\n      pc5\n    \n  \n  \n    \n      0\n      -4.241790\n      0.275651\n      0.358924\n      -0.495200\n      -0.275406\n    \n    \n      1\n      -2.030945\n      1.637908\n      0.070001\n      -0.686920\n      -0.911864\n    \n    \n      2\n      -1.854638\n      0.671051\n      1.212392\n      0.068783\n      0.847572\n    \n    \n      3\n      -1.925394\n      1.483075\n      -0.648590\n      0.746669\n      0.069074\n    \n    \n      4\n      -0.947741\n      -1.384563\n      0.557716\n      1.403352\n      -0.043060\n    \n  \n\n\n\n\n\ndf_2015_pca.plot.scatter(x=\"pc1\",y=\"pc2\")\nplt.show()\n\n\n\n\nLet’s plot the percentage variance explained vs the components. This plot would have to graphs with bars showing the vairance explained by each component and line showing the cumulative sum of the percent variance explained.\n\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\n\nax1.bar(range(len(pca_2015_model.explained_variance_ratio_)), \\\n        pca_2015_model.explained_variance_ratio_, alpha=0.5)\nax2.plot(pca_2015_model.explained_variance_ratio_.cumsum())\nax2.scatter(range(len(pca_2015_model.explained_variance_ratio_)), \\\n            pca_2015_model.explained_variance_ratio_.cumsum())\n\nax2.set_ylim([0,1])\n\nax1.set_ylabel(\"Percent Explained\")\nax2.set_ylabel(\"Cummulative Sum\")\nax1.set_xlabel(\"Principal Components\")\nplt.xticks(ticks=range(len(pca_2015_model.explained_variance_ratio_)), \\\n           labels=[\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\"])\nplt.show()"
  },
  {
    "objectID": "PCA_tutorial.html#pca-loadings",
    "href": "PCA_tutorial.html#pca-loadings",
    "title": "5  Principal Component Analysis",
    "section": "5.3 PCA Loadings",
    "text": "5.3 PCA Loadings\nThe loadings value range from -1 to 1 such that its value close to either end (1 or -1) is indicative of a strong influence of a particular feature to a specific component. Loadings can be thought of as analogous to correlation coefficient. To get the loadings we need to mutiple the eigenvectors with the square-root of egienvalues (reference).\n\npca_2015_model.explained_variance_\n\narray([5.01231878, 1.3206679 , 0.62418533, 0.45207762, 0.30851375])\n\n\n\nloadings2 = pca_2015_model.components_.T * np.sqrt(pca_2015_model.explained_variance_)\nloading_matrix = pd.DataFrame(loadings2, columns=['pc1', 'pc2','pc3','pc4','pc5'], index=df_2015.columns.values[1:-1])\nloading_matrix\n\n\n\n\n\n  \n    \n      \n      pc1\n      pc2\n      pc3\n      pc4\n      pc5\n    \n  \n  \n    \n      Days-to-heading\n      -0.369532\n      -0.835015\n      -0.178753\n      -0.282162\n      0.171208\n    \n    \n      Culm length (cm)\n      -0.826093\n      -0.280601\n      0.163783\n      -0.221699\n      -0.100626\n    \n    \n      Panicle No.\n      0.543720\n      -0.548766\n      0.591266\n      0.208845\n      -0.103117\n    \n    \n      Panicle length (cm)\n      -0.854466\n      -0.243123\n      -0.242804\n      0.360792\n      -0.108331\n    \n    \n      Rachis length (cm)\n      -0.913990\n      -0.156715\n      -0.072516\n      0.336950\n      0.071613\n    \n    \n      Primary branch No.\n      -0.804109\n      0.278571\n      0.331493\n      0.028416\n      0.409365\n    \n    \n      Sec. branch No.\n      -0.914241\n      0.172605\n      0.110709\n      -0.157018\n      -0.238994\n    \n    \n      Spikelet No.\n      -0.921839\n      0.229070\n      0.171628\n      -0.102605\n      -0.129926\n    \n  \n\n\n\n\nThe signs for the loadings are arbitary. The absolute number represents the correlation with the corresponding pincipal component. The opposite signs for the components indicate negative correlation within the components."
  },
  {
    "objectID": "PCA_tutorial.html#biplot",
    "href": "PCA_tutorial.html#biplot",
    "title": "5  Principal Component Analysis",
    "section": "5.4 Biplot",
    "text": "5.4 Biplot\nBiplot is a useful visualization methods for PCA as gives information about relationship of different features with principal components. This plot comprises of two plots (hence the name): 1. A scatter plot showing transformed data along the first two principle components 2. PCA loadings as arrows\nVisual analysis of this plot can give information about the extent to which different features affect a specific principal components.\n\ncoeff = loading_matrix.values\nplt.scatter(pca_2015_fit[:,0], pca_2015_fit[:,1], marker='.', alpha=0.5)\nfor i in range(8):\n    plt.arrow(0, 0, coeff[i,0], coeff[i,1], color = 'k', \\\n              alpha=0.5, linestyle = '-',linewidth = 1.5, overhang=0.2)\n\n\n\n\nCustomising the biplot to match the figure given in the manuscript. The circle in the graph represents 100% variance explained.\n\n#coeff = np.transpose(pca_2015.components_[0:2, :])\ncoeff = loading_matrix.values\nfor i in range(8):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color = 'k', alpha = 0.9, \\\n                  linestyle = '-',linewidth = 1.5, overhang=0.2)\n        plt.text(coeff[i,0]*1.25, coeff[i,1]*1.25, \\\n                 df_2015.columns.values[1:-1][i], color = 'k', ha = 'center', \\\n                 va = 'center',fontsize=10)\ncircle1 = plt.Circle((0, 0), 1, color='grey',fill=False)\nplt.gca().add_patch(circle1)\nplt.plot([0, 0], [-1, 1], color='grey', linestyle='-', linewidth=1)\nplt.plot([-1, 1], [0, 0], color='grey', linestyle='-', linewidth=1)\n\nplt.gca().set_box_aspect(1)\n\nplt.xlabel(f\"PC1 ({pca_2015_model.explained_variance_ratio_[0]*100:.2f}%)\")\nplt.ylabel(f\"PC2 ({pca_2015_model.explained_variance_ratio_[1]*100:.2f}%)\")\n\nplt.yticks(ticks=[-1,-0.5,0,0.5,1])\nplt.title(\"PCA loading for the 2015 data\")\n\nplt.show()\n\n\n\n\nCompare the above graph with Figure 1B of this manuscript. For a discussion on the results see the supplementary material.\n\nExercise\nCreate a figure showing PCA loadings for the 2014 data."
  }
]